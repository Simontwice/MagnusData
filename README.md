This repo contains the results of the data mining described <TODO: link paper>. You can download/stream the dataset used in the paper at <TODO: link HF>. The code in this repo can be used to process raw data included in <TODO: link HF> and generate new datasets, which can be used for premise selection, proof generation and text retrieval tasks.

# Introduction

We are proud to present the dataset generating scripts and raw data used to generate a premise selection dataset described in <TODO: paper>. The premise selection model trained on the dataset achieved state-of-the-art 71% proof rate on the [PISA](http://aitp-conference.org/2021/abstract/paper_17.pdf) benchmark and 37.3% on [miniF2F](https://github.com/openai/miniF2F).

# Raw data overview

The raw data used to generate datasets is a collection of proof trajectories - sequences that contain information about the commands called to Isabelle (the proof assistant) and the corresponding proof states.

Each proof trajectory corresponds to a unique proof of a problem, written by a human expert or generated automatically with Sledgehammer. The proofs generated by Sledgehammer are alternatives to the original human proofs, so some trajectories in the raw data come from the same original problems.

## Data Structure

Let's see how a proof in Isabelle is transformed into raw data. 

For the following proof:
```
theorem identity1: fixes f :: "nat \<Rightarrow> nat"
assumes fff: "\<And>n. f(f(n)) < f(Suc(n))"
shows "f(n) = n"
proof -
  { fix m n have key: "n \<le> m \<Longrightarrow> n \<le> f(m)"
    proof(induct n arbitrary: m)
      case 0 show ?case by simp
    [...]
    qed }
  hence "\<And>n. n \<le> f(n)" by simp
  hence "\<And>n. f(n) < f(Suc n)" by(metis fff order_le_less_trans)
  hence "f(n) < n+1" by (metis fff lift_Suc_mono_less_iff[of f] Suc_eq_plus1)
  with \<open>n \<le> f(n)\<close> show "f n = n" by arith
qed
```
We get the following trajectory

```
{
  'statement': 'theorem identity1: fixes f :: "nat \\<Rightarrow> nat"
assumes fff: "\\<And>n. f(f(n)) < f(Suc(n))"
shows "f(n) = n"'
  'transitions': <list_of_transitions>
}
```
where each transition in `<list_of_transitions>` corresponds to a single proof step (we do not include the full list for brevity).

A transition that corresponds to the step `by (metis fff lift_Suc_mono_less_iff[of f] Suc_eq_plus1)` called above is
```
  'state': 'proof (prove)\n using this:\n   f ?n < f (Suc ?n) \n goal (1 subgoal):\n  1. f n < n + 1'
  'step': 'by (metis fff lift_Suc_mono_less_iff[of f] Suc_eq_plus1)'
  'premises': {'fff': ['local.fff', ' fff: fixes n :: "nat" shows "f (f n) < f (Suc n)"'], 
  'lift_Suc_mono_less_iff': ['Nat.order.lift_Suc_mono_less_iff', ' lift_Suc_mono_less_iff: fixes less_eq :: "\'a \\<Rightarrow> \'a \\<Rightarrow> bool"   and less :: "\'a \\<Rightarrow> \'a \\<Rightarrow> bool"   and f :: "nat \\<Rightarrow> \'a"   and n :: "nat"   and m :: "nat" assumes "class.order less_eq less"   and "\\<And>n. less (f n) (f (Suc n))" shows "less (f n) (f m) = (n < m)"'],
  'Suc_eq_plus1': ['Nat.Suc_eq_plus1', ' Suc_eq_plus1: fixes n :: "nat" shows "Suc n = n + 1"']}
```
Hence the structure of a trajectory is:
```
Trajectory:
{
  'statement': str,
  'transitions': List[Transition]
}
```
where
```
Transition: 
{
  'state': str,
  'step': str,
  'premises': Dict[str,Tuple[str,str]]
}
```
In 'premises', the keys are names of the premises referenced in the proof step, whereas the values are lists, where the first element is the name of the premise in the lemma library, and the second element is the statement of the premise.

## Dataset generation

In order to generate your own dataset from the raw data, first run `setup.sh` to set up a virtual environment and install dependencies.
```
chmod +x setup.sh
./setup.sh
```

Then, download the selected raw files from <TODO: link HF>. We include a discussion of the source and extraction methods for all the raw data files below.

Finally, run the following script to generate the data in a `JSON` format compliant with Huggingface's [Datasets](https://huggingface.co/docs/datasets/index):
```
python generation_script.py --file-paths <paths> --out-path <out_path> --forbidden-statements-path PISA_test_theorems.json
```

### Overview of raw data files
All files are available for download at <TODO: HF repo>

The data is divided into two categories: `human data` and `machine_generated_data`.

`Human data` comes directly from human proofs, while `machine_generated_data` consists of alternative proofs generated by [Sledgehammer](https://isabelle.in.tum.de/website-Isabelle2009-1/sledgehammer.html).

The machine generated data is further divided into `gen1` and `gen2`, which differs in Sledgehammer configurations used for mining: `gen1` proofs use a less diverse set of tactics, but sometimes have a higher chance of producing a successful proof.

`Mode_1` and `mode_2` refer to the proof generation approach: proofs in `mode_1` were generated by replacing the last step of a successful human proof with a Sledgehammer step, while `mode_2` attempted to replace the entire proof with a single Sledgehammer step.

Note that human proofs are usually nested, so those two approaches considered all sub-proofs separately as well.

The last distinction, `minimal` and `non-minimal` refers again to Sledgehammer configuration: proofs in `minimal` were found by first finding a `non-minimal` version, then trying to remove unnecessary premises. Thus, `non-minimal` proofs contain more premises, but some of them might be redundant. For batch-contrastive learning there is a case to be made for both approaches.
### Languages

All information contained in this dataset is written in English and using the Isabelle syntax, which represents mathematical expressions using syntax similar to LaTeX.

### Source Data
The dataset was created using the proofs included in the [Archive of Formal Proofs](https://www.isa-afp.org/) and the Standard library included in the [Isabelle](https://isabelle.in.tum.de/) 2021-1 distribution.

### Known Limitations

The data included in this dataset is mostly untyped, meaning that there is little information about the objects referenced in the statement or premise statements. Adding type information would be a valuable contribution.

### Citation
<TODO: cite arxiv paper>

### Acknowledgements
<TODO: add Jin to contributors>
<TODO: acknowledge that mining was done with compute from google>
<TODO: mention that PISA was used to mine the raw data>
